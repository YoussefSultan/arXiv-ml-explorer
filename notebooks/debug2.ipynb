{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "import xml.etree.ElementTree as ET\n",
    "import dateutil.parser\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "class ArxivAPI:\n",
    "    \"\"\"\n",
    "    The ArxivAPI class is responsible for fetching papers from the ArXiv API based on a specific URL.\n",
    "    \n",
    "    Attributes:\n",
    "        url (str): The URL to fetch papers from the ArXiv API.\n",
    "        papers (list): A list to store the fetched papers, each represented as a dictionary.\n",
    "    \n",
    "    Methods:\n",
    "        fetch_papers(): Makes a request to the ArXiv API, parses the response to extract required details \n",
    "                        from each paper, and stores the details in the 'papers' list. Returns a DataFrame\n",
    "                        created from the 'papers' list.\n",
    "    \"\"\"\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.papers = []\n",
    "        \n",
    "    def fetch_papers(self):\n",
    "        response = requests.get(self.url)\n",
    "        root = ET.fromstring(response.content)\n",
    "        namespaces = {'atom': 'http://www.w3.org/2005/Atom', 'arxiv': 'http://arxiv.org/schemas/atom'}\n",
    "\n",
    "        for entry in root.findall('atom:entry', namespaces):\n",
    "            paper = {}\n",
    "            paper['title'] = entry.find('atom:title', namespaces).text\n",
    "            paper['summary'] = entry.find('atom:summary', namespaces).text\n",
    "            paper['date'] = dateutil.parser.parse(entry.find('atom:published', namespaces).text)\n",
    "            self.papers.append(paper)\n",
    "\n",
    "        return pd.DataFrame(self.papers)\n",
    "\n",
    "\n",
    "class ProcessingData:\n",
    "    \"\"\"\n",
    "    The ProcessingData class is used to process a DataFrame of papers, including transforming paper \n",
    "    summaries into tf-idf vectors, fitting an NMF model to extract topics, and mapping each paper to its topic.\n",
    "\n",
    "    Attributes:\n",
    "        df (DataFrame): The DataFrame of papers to be processed.\n",
    "        n_topics (int): The number of topics to extract from the NMF model.\n",
    "        n_top_words (int): The number of top words to consider in each topic.\n",
    "        n_connected_words (int): The number of words to include in the summary of each topic.\n",
    "        stop_words (list): A list of words to exclude during the tf-idf transformation.\n",
    "\n",
    "    Methods:\n",
    "        process(): Performs the tf-idf transformation, fits the NMF model, assigns each paper to its topic, \n",
    "                   and creates a summary for each topic. Returns a DataFrame with additional columns for the topic\n",
    "                   and the topic summary. The final grouping of words of the topic is based on a random start index\n",
    "                   position of 5 connected words from the n_top_words.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, n_topics=10, n_top_words=10, n_connected_words=5):\n",
    "        self.df = df\n",
    "        self.n_topics = n_topics\n",
    "        self.n_top_words = n_top_words\n",
    "        self.n_connected_words = n_connected_words\n",
    "        self.stop_words = list(ENGLISH_STOP_WORDS)\n",
    "        self.stop_words.extend(['data','tasks','task','models','node','machine', 'datasets'])\n",
    "\n",
    "    def process(self):\n",
    "        vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=self.stop_words)\n",
    "        tfidf = vectorizer.fit_transform(self.df['summary'])\n",
    "\n",
    "        nmf = NMF(n_components=self.n_topics, random_state=1, max_iter=2000).fit(tfidf)\n",
    "        self.df['topic'] = np.argmax(nmf.transform(tfidf), axis=1) + 1\n",
    "        self.df['year'] = self.df['date'].dt.year\n",
    "\n",
    "        _dict = {}\n",
    "        for topic_idx, topic in enumerate(nmf.components_):\n",
    "            top_word_indices = topic.argsort()[::-1][:self.n_top_words]\n",
    "            top_words = [vectorizer.get_feature_names_out()[i] for i in top_word_indices]\n",
    "            start_index = random.randint(0, len(top_words) - self.n_connected_words)\n",
    "            connected_words = top_words[start_index : start_index + self.n_connected_words]\n",
    "            _dict[f\"{topic_idx + 1}\"] = ' '.join(connected_words)\n",
    "\n",
    "        topics_df = pd.DataFrame(_dict, index=[0]).T.reset_index()\n",
    "        topics_df.columns = ['topic','topics']\n",
    "        topics_df.topic = topics_df.topic.astype('int')\n",
    "\n",
    "        return self.df.merge(topics_df, on='topic', how='left')\n",
    "\n",
    "\n",
    "class Visualize:\n",
    "    \"\"\"\n",
    "    The Visualize class is used to create a visualization of the distribution of topics over time.\n",
    "\n",
    "    Attributes:\n",
    "        df (DataFrame): The DataFrame of papers, each with an assigned topic and a topic summary.\n",
    "\n",
    "    Methods:\n",
    "        plot(): Creates and displays a stacked bar chart showing the distribution of topics over time.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        #self.color_dict = {'topic1':'#1f77b4', 'topic2':'#ff7f0e', 'topic3':'#2ca02c', 'topic4':'#d62728', 'topic5':'#9467bd', ...}\n",
    "\n",
    "    def plot(self,number, color):\n",
    "        self.df['date'] = pd.to_datetime(self.df['date'])\n",
    "        df_grouped = self.df.groupby([self.df['date'].dt.date, 'topics']).size().reset_index(name='count')\n",
    "        df_wide = df_grouped.pivot(index='date', columns='topics', values='count').reset_index().fillna(0)\n",
    "\n",
    "        fig = px.bar(df_wide, x='date', y=df_wide.columns[1:],\n",
    "                     labels={'value':'Frequency', 'date':'Date', 'variable':'Topic Summary'},\n",
    "                     title=f'Topic Distribution Over Time for the Last {number} Machine Learning Papers on ArXiv',\n",
    "                     color_discrete_sequence=color)\n",
    "        fig.update_layout(barmode='stack')\n",
    "        return fig\n",
    "    \n",
    "    def plot2(self, number, color):\n",
    "        self.df['date'] = pd.to_datetime(self.df['date'])\n",
    "        df_grouped = self.df.groupby([self.df['date'].dt.date, 'topics']).size().reset_index(name='count')\n",
    "        df_total_count_per_date = self.df.groupby([self.df['date'].dt.date]).size().reset_index(name='count')\n",
    "        df_counts = df_grouped.merge(df_total_count_per_date, on='date',how='left')\n",
    "        df_counts['ratio'] = df_counts.count_x / df_counts.count_y\n",
    "        df_wide_proportions = df_counts.pivot(index='date', columns='topics', values='ratio').reset_index().fillna(0)\n",
    "\n",
    "        fig = px.bar(df_wide_proportions, x='date', y=df_wide_proportions.columns[1:],\n",
    "                    labels={'value':'Proportion', 'date':'Date', 'variable':'Topic Summary'},\n",
    "                    title=f'Topic Proportion Over Time for the Last {number} Machine Learning Papers on ArXiv',\n",
    "                    color_discrete_sequence=color)\n",
    "        fig.update_layout(barmode='stack')\n",
    "        return fig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df2\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.17s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.dirname(os.getcwd()) + \"/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(os.path.dirname(os.getcwd()) + \"/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "     -------------------------------------- 977.5/977.5 kB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "hf_hub_download() missing 1 required positional argument: 'repo_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m# file to download\u001b[39;00m\n\u001b[0;32m     10\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpytorch_model.bin\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# adjust this to the actual file you want to download\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m file_local_path \u001b[39m=\u001b[39m hf_hub_download(repo_type\u001b[39m=\u001b[39;49mrepo_id, filename\u001b[39m=\u001b[39;49mfilename, token\u001b[39m=\u001b[39;49mtoken)\n\u001b[0;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile has been downloaded to \u001b[39m\u001b[39m{\u001b[39;00mfile_local_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\water\\OneDrive\\Documents\\GitHub\\arXiv-ml-explorer\\.conda\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: hf_hub_download() missing 1 required positional argument: 'repo_id'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# your hugging face token\n",
    "token = 'your-huggingface-token'\n",
    "\n",
    "# repository id\n",
    "repo_id = 'https://huggingface.co/meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "# file to download\n",
    "filename = 'pytorch_model.bin'  # adjust this to the actual file you want to download\n",
    "\n",
    "file_local_path = hf_hub_download(repo_id=repo_id, filename=filename, token=token)\n",
    "\n",
    "print(f\"File has been downloaded to {file_local_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.75s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import os \n",
    "tokenizer = LlamaTokenizer.from_pretrained(os.path.dirname(os.getcwd()) + \"/Llama-2-7b-chat-hf\")\n",
    "model = LlamaForCausalLM.from_pretrained(os.path.dirname(os.getcwd()) + \"/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def askLLAMAv2(topic):\n",
    "    input_text = \"\"\"\n",
    "    _topic_\n",
    "    \"\"\".replace('_topic_',topic)\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    \"\"\"\n",
    "    temperature = 0.9\n",
    "    top_k = 50\n",
    "    top_p = 0.9\n",
    "    input_text = prompt + input_text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    generate_ids = model.generate(inputs.input_ids, do_sample=True, max_length=200, \n",
    "                                temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "\n",
    "    result = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    final_result = result[0]\n",
    "    final_result = final_result.replace(input_text, '').strip()\n",
    "\n",
    "    return final_result\n",
    "askLLAMAv2(\"This research paper Conformer LLMs -- Convolution Augmented Large Language Models was published 21 days ago. Tell us some insights based causal variables latent effect inference treatment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\n    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \\n\\n    \\n    Summarize the following tokens into a 5 word category\\n    (adversarial attacks robustness training robust defense backdoor model examples_)\\n    5 words: Adversarial attacks, robustness training, robust defense, backdoor model, examples_\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = 'adversarial attacks robustness training robust defense backdoor model examples'\n",
    "askLLAMAv2(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generate_ids[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
